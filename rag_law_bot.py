import os
from dotenv import load_dotenv

# ==============================
# 1. è¯»å–ç¯å¢ƒå˜é‡ & DeepSeek è¡¥ä¸
# ==============================
load_dotenv()

if os.getenv("OPENAI_API_KEY") is None:
    raise ValueError("æ²¡æœ‰æ‰¾åˆ° OPENAI_API_KEYï¼Œè¯·æ£€æŸ¥ .env æ–‡ä»¶æ˜¯å¦é…ç½®æ­£ç¡®ã€‚")

import openai

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.base_url = os.getenv("OPENAI_BASE_URL", "https://api.deepseek.com/v1")

print(f"å·²ä¸º OpenAI SDK è®¾ç½® base_url = {openai.base_url}")
print("âœ… å·²è¯»å–åˆ° OPENAI_API_KEYï¼Œå‡†å¤‡åˆå§‹åŒ– LLM ä¸å‘é‡æ¨¡å‹...")

# ==============================
# 2. LlamaIndex & DeepSeek å…¼å®¹è®¾ç½®
# ==============================
from llama_index.core import (
    SimpleDirectoryReader,
    VectorStoreIndex,
    Settings,
)
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.openai import OpenAI
import llama_index.llms.openai.utils as openai_utils

# ç»™ LlamaIndex æ‰“è¡¥ä¸ï¼Œè®©å®ƒè®¤è¯† deepseek-chat
_orig_ctx_func = openai_utils.openai_modelname_to_contextsize


def _patched_openai_modelname_to_contextsize(model_name: str) -> int:
    if model_name.startswith("deepseek"):
        # DeepSeek ä¸Šä¸‹æ–‡ä¸€èˆ¬æ˜¯ 8K æˆ– 16Kï¼Œè¿™é‡Œä¿å®ˆç»™ 8192
        return 8192
    return _orig_ctx_func(model_name)


openai_utils.openai_modelname_to_contextsize = _patched_openai_modelname_to_contextsize
print("ğŸ”§ å·²ä¸º LlamaIndex æ‰“è¡¥ä¸ï¼Œä½¿å…¶æ”¯æŒ deepseek-chat æ¨¡å‹ã€‚")

# é…ç½®ä¸­æ–‡å‘é‡æ¨¡å‹ï¼ˆBGEï¼Œå°å‹ä¸­æ–‡ embeddingï¼‰
EMBED_MODEL_NAME = "BAAI/bge-small-zh-v1.5"
print(f"ğŸ“¦ æ­£åœ¨åŠ è½½å‘é‡æ¨¡å‹: {EMBED_MODEL_NAME} ...")
embed_model = HuggingFaceEmbedding(model_name=EMBED_MODEL_NAME)
Settings.embed_model = embed_model

# é…ç½® DeepSeek ä½œä¸º LLM
llm = OpenAI(
    model="deepseek-chat",
    temperature=0.2,
)
Settings.llm = llm
print("ğŸ¤– å·²é…ç½® deepseek-chat ä½œä¸ºå¯¹è¯æ¨¡å‹ã€‚")

# ==============================
# 3. åˆ†åˆ«åŠ è½½ã€Œæ³•æ¡åº“ã€å’Œã€Œæ¡ˆä¾‹åº“ã€
# ==============================
STATUTE_DIR = "./data/statutes"
CASE_DIR = "./data/cases"

# --- æ³•æ¡åº“ ---
if not os.path.exists(STATUTE_DIR):
    raise ValueError("æ²¡æœ‰æ‰¾åˆ° ./data/statutes ç›®å½•ï¼Œè¯·åˆ›å»ºå¹¶æŠŠã€ŠåŠ³åŠ¨åˆåŒæ³•ã€‹ç­‰æ³•æ¡ txt æ”¾è¿›å»ã€‚")

print(f"ğŸ“š æ­£åœ¨åŠ è½½æ³•æ¡æ–‡æ¡£ {STATUTE_DIR} ...")
statute_docs = SimpleDirectoryReader(STATUTE_DIR).load_data()
print(f"å·²åŠ è½½æ³•æ¡æ–‡æ¡£æ•°é‡: {len(statute_docs)}")

statute_index = VectorStoreIndex.from_documents(statute_docs)
statute_retriever = statute_index.as_retriever(similarity_top_k=5)  # æ£€ç´¢å°‘é‡é«˜ç›¸å…³æ³•æ¡


# --- æ¡ˆä¾‹åº“ ---
case_retriever = None
best_case_enabled = False

if os.path.exists(CASE_DIR) and os.listdir(CASE_DIR):
    print(f"ğŸ“š æ­£åœ¨åŠ è½½æ¡ˆä¾‹æ–‡æ¡£ {CASE_DIR} ...")
    case_docs = SimpleDirectoryReader(CASE_DIR).load_data()
    print(f"å·²åŠ è½½æ¡ˆä¾‹æ–‡æ¡£æ•°é‡: {len(case_docs)}")
    case_index = VectorStoreIndex.from_documents(case_docs)
    # è¿™é‡Œ top_k ç¨å¾®æ”¾å¤§ä¸€ç‚¹ï¼Œåªç”¨äºé€‰å‡ºã€Œæœ€åƒçš„ 1 ä¸ªã€
    case_retriever = case_index.as_retriever(similarity_top_k=12)
    best_case_enabled = True
else:
    print("âš  æœªæ‰¾åˆ° ./data/cases ç›®å½•æˆ–ç›®å½•ä¸ºç©ºï¼Œå°†æš‚ä¸å¯ç”¨ç±»æ¡ˆæ£€ç´¢ã€‚")

print("ğŸ§  å‘é‡ç´¢å¼•æ„å»ºå®Œæˆï¼å¯ä»¥å¼€å§‹æé—®äº†ï½")


# ==============================
# 4. æ„é€ æœ€ç»ˆå›ç­”çš„ Prompt
# ==============================
def build_final_prompt(question: str, statute_nodes, case_node_text: str | None) -> str:
    """æŠŠæ£€ç´¢åˆ°çš„æ³•æ¡ç‰‡æ®µ & æ¡ˆä¾‹ç‰‡æ®µå¡è¿›ä¸€ä¸ªæ€» Promptï¼Œè®© LLM æŒ‰å›ºå®šç»“æ„å›ç­”ã€‚"""

    statute_context_parts = []
    for i, sn in enumerate(statute_nodes[:3], 1):
        content = sn.node.get_content().strip()
        statute_context_parts.append(f"ã€æ³•æ¡ç‰‡æ®µ{i}ã€‘\n{content}\n")
    statute_context = "\n".join(statute_context_parts) if statute_context_parts else "ï¼ˆæœªæ£€ç´¢åˆ°æ˜æ˜¾ç›¸å…³çš„æ³•æ¡ç‰‡æ®µï¼‰"

    case_context = case_node_text.strip() if case_node_text else "ï¼ˆæ— æ˜æ˜¾ç›¸å…³æ¡ˆä¾‹ï¼Œä»…ä¾›ä¸€èˆ¬æ€§å›ç­”ï¼‰"

    prompt = f"""
ä½ æ˜¯ä¸€åç²¾é€šä¸­å›½åŠ³åŠ¨æ³•çš„å¾‹å¸ˆåŠ©æ‰‹ï¼Œè¯·åŸºäºç»™å®šçš„ã€æ³•æ¡ææ–™ã€‘å’Œã€ç±»æ¡ˆææ–™ã€‘å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œ
å¹¶æŒ‰ç…§è¦æ±‚çš„ç»“æ„è¾“å‡ºï¼Œæ³¨æ„ä¸è¦è¾“å‡ºä»»ä½•æŠ€æœ¯ç»†èŠ‚ï¼ˆå¦‚ TopKã€score ç­‰ï¼‰ã€‚

ã€ç”¨æˆ·é—®é¢˜ã€‘
{question}

ã€æ³•æ¡ææ–™ã€‘
{statute_context}

ã€ç±»æ¡ˆææ–™ã€‘
{case_context}

è¯·ç”¨ä¸­æ–‡è¾“å‡ºï¼Œç»“æ„ä¸¥æ ¼å¦‚ä¸‹ï¼ˆæ ‡é¢˜å’Œåºå·éƒ½è¦ä¿ç•™ï¼‰ï¼š

1. ç»“è®ºä¸åˆ†æï¼š
- å…ˆç”¨ 1ï½3 å¥è¯ç›´æ¥ç»™å‡ºæ˜ç¡®ç»“è®ºï¼ˆä¾‹å¦‚ï¼šæ˜¯å¦è¿æ³•ã€èƒ½å¦ä¸»å¼ åŒå€å·¥èµ„ã€æ˜¯å¦æ„æˆåŠ³åŠ¨å…³ç³»ç­‰ï¼‰ã€‚
- å†ç”¨ 3ï½6 å¥è¯è¿›è¡Œç®€è¦çš„æ³•å¾‹åˆ†æï¼Œé‡ç‚¹è¯´æ˜ï¼š
  Â· é€‚ç”¨çš„æ˜¯å“ªäº›æ³•å¾‹æ¡æ–‡ï¼ˆå†™å‡ºæ¡æ¬¾å·ï¼Œå¦‚ã€ŠåŠ³åŠ¨åˆåŒæ³•ã€‹ç¬¬82æ¡ï¼‰ï¼Œ
  Â· å¯¹å½“äº‹äººæœ‰åˆ©å’Œä¸åˆ©çš„å› ç´ å„æ˜¯ä»€ä¹ˆï¼Œ
  Â· å¦‚æœ‰å‰ææ¡ä»¶æˆ–ä¾‹å¤–æƒ…å†µï¼Œä¹Ÿä¸€å¹¶è¯´æ˜ã€‚

2. é€‚ç”¨æ³•æ¡åŠæ¡æ–‡å†…å®¹ï¼š
- åªåˆ—å‡ºä¸ä½ ç»“è®ºç›´æ¥ç›¸å…³çš„ 2ï½4 æ¡å…³é”®æ³•æ¡ã€‚
- æ ¼å¼ç¤ºä¾‹ï¼š
  ï¼ˆ1ï¼‰ã€ŠåŠ³åŠ¨åˆåŒæ³•ã€‹ç¬¬82æ¡ã€æœªè®¢ç«‹ä¹¦é¢åŠ³åŠ¨åˆåŒçš„æ³•å¾‹åæœã€‘ï¼šâ€¦â€¦ï¼ˆå¼•ç”¨å…³é”®æ¡æ–‡åŸæ–‡æˆ–é«˜åº¦æ¦‚æ‹¬ï¼Œä½†ä¸è¦è¶…è¿‡ 200 å­—ï¼‰
- å¦‚æœ‰éœ€è¦ï¼Œå¯ä»¥è¡¥å……ã€ŠåŠ³åŠ¨äº‰è®®è°ƒè§£ä»²è£æ³•ã€‹ã€Šæ°‘æ³•å…¸ã€‹ã€Šå…¬å¸æ³•ã€‹ç­‰ç›¸å…³æ¡æ¬¾ï¼Œä½†ä¸è¦å †ç Œæ— å…³æ¡æ–‡ã€‚

3. ç±»æ¡ˆå‚è€ƒï¼ˆå¦‚æœ‰ï¼‰ï¼š
- å¦‚æœã€ç±»æ¡ˆææ–™ã€‘ä¸é—®é¢˜é«˜åº¦ç±»ä¼¼ï¼Œè¯·ç”¨ 1ï½2 æ®µè¯æ¦‚æ‹¬ï¼š
  Â· æ¡ˆæƒ…è¦ç‚¹ï¼ˆå½“äº‹äººèº«ä»½ã€æ ¸å¿ƒäº‰è®®ï¼‰ï¼›
  Â· è£åˆ¤ç»“è®ºï¼ˆæ³•é™¢å¦‚ä½•è®¤å®šï¼‰ï¼›
  Â· å¯¹æœ¬é—®é¢˜çš„å¯ç¤ºï¼ˆç”¨ 2ï½3 ç‚¹ç®€è¦è¯´æ˜ï¼‰ã€‚
- å¦‚æœã€ç±»æ¡ˆææ–™ã€‘ç›¸å…³æ€§ä¸é«˜ï¼Œè¯·ç»Ÿä¸€å†™ï¼š
  â€œæœ¬é—®é¢˜æš‚æ— ç‰¹åˆ«è´´è¿‘çš„å…¸å‹æ¡ˆä¾‹ï¼Œä»…èƒ½ä½œä¸€èˆ¬æ€§å‚è€ƒï¼Œå…·ä½“å¤„ç†ä»éœ€ç»“åˆä¸ªæ¡ˆäº‹å®ã€‚â€

è¦æ±‚ï¼š
- å…¨ç¨‹ä¸è¦å‡ºç°â€œTopKâ€â€œscoreâ€â€œsource_nodesâ€ç­‰æŠ€æœ¯å­—æ®µã€‚
- ä¸è¦ç…§æ¬åŸæ–‡ä¸­çš„â€œã€ç†ç”±ã€‘â€æ ‡é¢˜ï¼Œè€Œæ˜¯æ”¹å†™æˆè‡ªç„¶çš„è¯´ç†æ®µè½ã€‚
- è¯­è¨€é£æ ¼ä»¥ä¸“ä¸šå¾‹å¸ˆé£æ ¼ä¸ºä¸»ï¼Œä½†å°½é‡è®©éæ³•å¾‹ä¸“ä¸šäººå£«ä¹Ÿèƒ½çœ‹æ‡‚ã€‚
"""
    return prompt


# ==============================
# 5. å‘½ä»¤è¡Œå¯¹è¯å¾ªç¯
# ==============================
while True:
    user_input = input("\nğŸ’¬ è¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼ˆè¾“å…¥ q é€€å‡ºï¼‰ï¼š\n> ").strip()
    if user_input.lower() in ["q", "quit", "exit"]:
        print("ğŸ‘‹ å·²é€€å‡ºï¼Œå†è§ï½")
        break

    if not user_input:
        continue

    try:
        # 1ï¼‰å…ˆåˆ†åˆ«ä»ã€Œæ³•æ¡ç´¢å¼•ã€å’Œã€Œæ¡ˆä¾‹ç´¢å¼•ã€æ£€ç´¢
        statute_nodes = statute_retriever.retrieve(user_input)

        best_case_text = None
        if best_case_enabled and case_retriever is not None:
            case_nodes = case_retriever.retrieve(user_input)
            if case_nodes:
                # åªé€‰æœ€ç›¸ä¼¼çš„ 1 ä¸ªæ¡ˆä¾‹
                best_case_text = case_nodes[0].node.get_content().strip()

        # 2ï¼‰ç”¨ä¸€ä¸ªæ€» Prompt è®© LLM æŒ‰ç»“æ„æ•´åˆå›ç­”
        final_prompt = build_final_prompt(user_input, statute_nodes, best_case_text)
        final_resp = llm.complete(final_prompt)

        print("\n====== æ¨¡å‹å›ç­” ======\n")
        print(final_resp.text.strip())

    except Exception as e:
        print("âŒ æŸ¥è¯¢å‡ºé”™ï¼š", e)
