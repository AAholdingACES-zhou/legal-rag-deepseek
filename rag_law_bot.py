import os
from dotenv import load_dotenv

# 1. å…ˆåŠ è½½ .env
load_dotenv()

if os.getenv("OPENAI_API_KEY") is None:
    raise ValueError("æ²¡æœ‰æ‰¾åˆ° OPENAI_API_KEYï¼Œè¯·æ£€æŸ¥ .env æ–‡ä»¶æ˜¯å¦é…ç½®æ­£ç¡®ã€‚")

# 2. å¼ºåˆ¶ OpenAI SDK ä½¿ç”¨ DeepSeek APIï¼ˆå…³é”®è¡¥ä¸ï¼‰
import openai

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.base_url = os.getenv("OPENAI_BASE_URL", "https://api.deepseek.com/v1")

print(f"å·²ä¸º OpenAI SDK è®¾ç½® base_url = {openai.base_url}")
print("âœ… å·²è¯»å–åˆ° OPENAI_API_KEYï¼Œå‡†å¤‡åˆå§‹åŒ– LLM ä¸å‘é‡æ¨¡å‹...")

from llama_index.core import (
    SimpleDirectoryReader,
    VectorStoreIndex,
    Settings,
)
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.openai import OpenAI
import llama_index.llms.openai.utils as openai_utils

print("âœ… å·²è¯»å–åˆ° OPENAI_API_KEYï¼Œå‡†å¤‡åˆå§‹åŒ– LLM ä¸å‘é‡æ¨¡å‹...")


# 2. ç»™ LlamaIndex æ‰“ä¸€ä¸ªâ€œå°è¡¥ä¸â€ï¼Œè®©å®ƒè®¤è¯† deepseek-chat
_orig_ctx_func = openai_utils.openai_modelname_to_contextsize


def _patched_openai_modelname_to_contextsize(model_name: str) -> int:
    # å¯¹ deepseek ç³»åˆ—æ¨¡å‹ï¼Œè¿”å›ä¸€ä¸ªå›ºå®šçš„ context window
    if model_name.startswith("deepseek"):
        # DeepSeek å®˜æ–¹ä¸Šä¸‹æ–‡ä¸€èˆ¬æ˜¯ 8K æˆ– 16Kï¼Œè¿™é‡Œä¿å®ˆç»™ 8192
        return 8192
    # å…¶ä»–æ¨¡å‹ä¾ç„¶èµ°åŸæ¥çš„é€»è¾‘
    return _orig_ctx_func(model_name)


openai_utils.openai_modelname_to_contextsize = _patched_openai_modelname_to_contextsize
print("ğŸ”§ å·²ä¸º LlamaIndex æ‰“è¡¥ä¸ï¼Œä½¿å…¶æ”¯æŒ deepseek-chat æ¨¡å‹ã€‚")


# 3. é…ç½®ä¸­æ–‡å‘é‡æ¨¡å‹ï¼ˆBGEï¼Œå°å‹ä¸­æ–‡ embeddingï¼‰
EMBED_MODEL_NAME = "BAAI/bge-small-zh-v1.5"
print(f"ğŸ“¦ æ­£åœ¨åŠ è½½å‘é‡æ¨¡å‹: {EMBED_MODEL_NAME} ...")
embed_model = HuggingFaceEmbedding(model_name=EMBED_MODEL_NAME)

Settings.embed_model = embed_model

# 4. é…ç½® DeepSeek ä½œä¸º LLMï¼ˆé€šè¿‡ OpenAI å…¼å®¹åè®®ï¼‰
#    æ³¨æ„ï¼šè¿™é‡Œçš„ model å°±æ˜¯ deepseek çš„æ¨¡å‹å
llm = OpenAI(
    model="deepseek-chat",
    temperature=0.2,
)
Settings.llm = llm
print("ğŸ¤– å·²é…ç½® deepseek-chat ä½œä¸ºå¯¹è¯æ¨¡å‹ã€‚")


# 5. ä» ./data ç›®å½•åŠ è½½æ³•å¾‹æ–‡æ¡£
DATA_DIR = "./data"
print(f"ğŸ“š æ­£åœ¨åŠ è½½æœ¬åœ°æ–‡æ¡£ {DATA_DIR} ...")
documents = SimpleDirectoryReader(DATA_DIR).load_data()
print(f"å·²åŠ è½½æ–‡æ¡£æ•°é‡: {len(documents)}")

# 6. æ„å»ºå‘é‡ç´¢å¼•
print("ğŸ§  æ­£åœ¨æ„å»ºå‘é‡ç´¢å¼•ï¼ˆVectorStoreIndexï¼‰...")
index = VectorStoreIndex.from_documents(documents)
print("âœ… ç´¢å¼•æ„å»ºå®Œæˆï¼å¯ä»¥å¼€å§‹æé—®äº†ï½")

# 7. åˆ›å»ºæŸ¥è¯¢å¼•æ“
query_engine = index.as_query_engine(
    similarity_top_k=3,          # æ¯æ¬¡ä»çŸ¥è¯†åº“é‡Œæ‰¾ 3 æ¡æœ€ç›¸è¿‘çš„æ³•æ¡/æ¡ˆä¾‹
    response_mode="compact",     # è¾“å‡ºç›¸å¯¹ç²¾ç®€
)


def pretty_print_response(resp):
    """ç¾åŒ–è¾“å‡ºï¼šæ­£æ–‡ + å¼•ç”¨ç‰‡æ®µ"""
    print("\n====== æ¨¡å‹å›ç­” ======\n")
    print(str(resp))

    # å±•ç¤ºå¼•ç”¨çš„æ³•æ¡ç‰‡æ®µï¼Œæ–¹ä¾¿ä½ æ ¸æŸ¥
    if getattr(resp, "source_nodes", None):
        print("\n====== å¼•ç”¨ç‰‡æ®µï¼ˆTop 3ï¼‰======")
        for i, sn in enumerate(resp.source_nodes[:3], 1):
            text = sn.node.get_content().strip()
            print(f"\n[{i}] score={sn.score:.3f}\n{text}\n")


# 8. ç®€å• REPL å¾ªç¯ï¼šåœ¨ç»ˆç«¯é‡Œå’Œ bot å¯¹è¯
while True:
    user_input = input("\nğŸ’¬ è¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼ˆè¾“å…¥ q é€€å‡ºï¼‰ï¼š\n> ").strip()
    if user_input.lower() in ["q", "quit", "exit"]:
        print("ğŸ‘‹ å·²é€€å‡ºï¼Œå†è§ï½")
        break

    if not user_input:
        continue

    try:
        resp = query_engine.query(user_input)
        pretty_print_response(resp)
    except Exception as e:
        print("âŒ æŸ¥è¯¢å‡ºé”™ï¼š", e)